# cf. https://github.com/triton-inference-server/model_analyzer/blob/main/docs/config.md
# Global config
model_repository: /models
checkpoint_directory: /perftest/checkpoints
perf_output: True
perf_output_path: /perftest/perf-analyzer-results
export_path: /perftest/model-analyzer-results
triton_launch_mode: local
override_output_model_repository: True
output_model_repository_path: /perftest/output-model-repository
client_protocol: grpc
objectives:
- perf_latency_p99
- perf_throughput
perf_analyzer_flags:
  percentile: 95
  latency-report-file: /perftest/perf-analyzer-latency-report.log
  shape:
# Global search space (client)
run_config_search_max_concurrency: 128
run_config_search_max_instance_count: 4

# Per-model config(s)
profile_models:
 # NOTE: ensemble model is not available for profiling
  # clip:
  #     model_config_parameters:
  #       max_batch_size: [1, 32, 128, 512]
  # clip-preprocess:
  #     model_config_parameters:
  #       max_batch_size: [1, 32, 128, 512]
  #     perf_analyzer_flags:
  #       input-data: /perftest/clip-preprocess-input-data.json
  clip-core:
      model_config_parameters:
        max_batch_size: [1, 32, 128, 512]
        dynamic_batching:
            max_queue_delay_microseconds: [1000, 10000, 100000, 1000000]
      perf_analyzer_flags:
        percentile: 95
        latency-report-file: /perftest/perf-analyzer-latency-report.log
        input-data: /perftest/clip-core-input-data.json
        # shape:
        #   - pixel_values:3,256,256
        #   - input_ids:77
        #   - attention_mask:77